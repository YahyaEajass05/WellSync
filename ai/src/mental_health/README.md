# Mental Wellness Prediction - Quick Start Guide

## ðŸš€ Quick Start

### Prerequisites
```bash
pip install pandas numpy scikit-learn matplotlib seaborn joblib
```

### 1. Train the Model
```bash
# From project root directory
cd D:\Python Projects\WellSync
.\.venv\Scripts\Activate.ps1
$env:PYTHONPATH="$PWD"
python ai/src/mental_health/train.py
```

**Training Process:**
- Loads dataset from `ai/data/ScreenTime_MentalWellness.csv`
- Performs advanced preprocessing & feature engineering
- Trains 8 baseline models
- Performs hyperparameter tuning on top 3 models
- Creates ensemble models (Voting & Stacking)
- Saves all models, visualizations, and reports
- **Duration:** ~40-50 seconds

**Output:**
- Best model RÂ² Score: **0.9426** â­
- Models saved in `ai/models/mental_health/`
- Visualizations in `ai/models/mental_health/visualizations/`
- Reports in `ai/models/mental_health/reports/`

---

### 2. Evaluate the Model
```bash
python ai/src/mental_health/evaluate.py
```

**Evaluation Process:**
- Loads best trained model
- Evaluates on test set
- Generates comprehensive metrics
- Creates evaluation visualizations
- Saves detailed evaluation report

**Metrics Displayed:**
- RÂ² Score, MAE, RMSE, MAPE
- Training vs Test comparison
- Overfitting analysis
- Feature importance rankings

---

## ðŸ“Š Model Performance Summary

### Best Model: Voting Ensemble
| Metric | Training | Test |
|--------|----------|------|
| **RÂ² Score** | 0.9738 | 0.9426 |
| **MAE** | 2.65 | 4.02 |
| **RMSE** | 3.28 | 4.97 |

### Cross-Validation (10-Fold)
- **RÂ² Score:** 0.9080 Â± 0.0364
- **MAE:** 4.55 Â± 0.45
- **RMSE:** 5.70 Â± 0.62

---

## ðŸŽ¯ Key Features

### Advanced Preprocessing
1. âœ… Missing value imputation
2. âœ… Outlier detection & handling
3. âœ… Robust scaling
4. âœ… Label encoding
5. âœ… Stratified splitting

### Feature Engineering (26 Total Features)
**Original Features (13):**
- age, gender, occupation, work_mode
- screen_time_hours, work_screen_hours, leisure_screen_hours
- sleep_hours, sleep_quality_1_5
- stress_level_0_10, productivity_0_100
- exercise_minutes_per_week, social_hours_per_week

**Engineered Features (13):**
- work_screen_ratio, leisure_screen_ratio
- sleep_efficiency
- work_life_balance, screen_sleep_ratio
- health_score
- stress_productivity_interaction
- age_group
- high_screen_time, excessive_work_screen
- screen_time_squared, stress_squared, sleep_squared

### Machine Learning Models
1. Random Forest (tuned)
2. Gradient Boosting (tuned)
3. Extra Trees (tuned)
4. Ridge Regression
5. Lasso Regression
6. ElasticNet
7. AdaBoost
8. K-Nearest Neighbors
9. **Voting Ensemble** â­ (BEST)
10. Stacking Ensemble

---

## ðŸ“ Project Structure

```
ai/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ScreenTime_MentalWellness.csv          # Dataset (400 samples)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mental_health/
â”‚       â”œâ”€â”€ best_model.pkl                     # Best model (Voting Ensemble)
â”‚       â”œâ”€â”€ tuned_random_forest.pkl
â”‚       â”œâ”€â”€ tuned_gradient_boosting.pkl
â”‚       â”œâ”€â”€ tuned_extra_trees.pkl
â”‚       â”œâ”€â”€ voting_ensemble.pkl
â”‚       â”œâ”€â”€ stacking_ensemble.pkl
â”‚       â”œâ”€â”€ preprocessors.pkl                  # Scaling, encoding objects
â”‚       â”œâ”€â”€ feature_names.pkl                  # Feature list
â”‚       â”œâ”€â”€ model_metadata.pkl                 # Training info
â”‚       â”œâ”€â”€ MODEL_SUMMARY.md                   # Detailed summary
â”‚       â”‚
â”‚       â”œâ”€â”€ visualizations/                    # All plots
â”‚       â”‚   â”œâ”€â”€ evaluation_predictions.png
â”‚       â”‚   â”œâ”€â”€ evaluation_residuals.png
â”‚       â”‚   â”œâ”€â”€ evaluation_error_distribution.png
â”‚       â”‚   â”œâ”€â”€ voting_ensemble_predictions.png
â”‚       â”‚   â””â”€â”€ voting_ensemble_residuals.png
â”‚       â”‚
â”‚       â””â”€â”€ reports/                           # Text reports
â”‚           â”œâ”€â”€ training_report_[timestamp].txt
â”‚           â””â”€â”€ evaluation_report_[timestamp].txt
â”‚
â””â”€â”€ src/
    â””â”€â”€ mental_health/
        â”œâ”€â”€ preprocess.py                      # Preprocessing pipeline
        â”œâ”€â”€ train.py                           # Training pipeline
        â”œâ”€â”€ evaluate.py                        # Evaluation module
        â””â”€â”€ README.md                          # This file
```

---

## ðŸ” Understanding the Code

### preprocess.py
**Function:** `preprocess_data(csv_path, save_preprocessors=True)`

**What it does:**
1. Loads dataset
2. Performs data quality checks
3. Creates 13 new engineered features
4. Handles missing values & outliers
5. Encodes categorical variables
6. Scales features using RobustScaler
7. Splits into train/test sets (80/20)
8. Saves preprocessing objects for deployment

**Returns:** X_train, X_test, y_train, y_test, feature_names, preprocessors

---

### train.py
**Function:** `train_model()`

**Pipeline Steps:**
1. **Preprocessing** - Feature engineering & data preparation
2. **Baseline Training** - Train 8 different algorithms
3. **Hyperparameter Tuning** - Optimize top 3 models
4. **Ensemble Creation** - Build Voting & Stacking ensembles
5. **Model Selection** - Choose best performing model
6. **Cross-Validation** - 10-fold validation
7. **Visualization** - Generate plots
8. **Save Artifacts** - Save models & reports

**Models Trained:**
- Random Forest â†’ Tuned Random Forest
- Gradient Boosting â†’ Tuned Gradient Boosting
- Extra Trees â†’ Tuned Extra Trees
- â†’ Voting Ensemble (combines top 3)
- â†’ Stacking Ensemble (meta-learner)

---

### evaluate.py
**Function:** `evaluate_saved_model(model_path)`

**Evaluation Steps:**
1. Load saved model
2. Load test data
3. Make predictions
4. Calculate comprehensive metrics
5. Perform overfitting analysis
6. Generate visualizations
7. Save evaluation report

**Metrics Calculated:**
- RÂ² Score (variance explained)
- MAE (average absolute error)
- RMSE (root mean square error)
- MAPE (percentage error)
- Residual statistics

---

## ðŸ“Š Visualizations Explained

### 1. Actual vs Predicted
- Scatter plot comparing actual and predicted values
- Red diagonal line = perfect prediction
- Points close to line = good predictions
- **Our result:** Points cluster tightly around the line âœ…

### 2. Residual Plot
- Shows prediction errors (residuals)
- Horizontal line at y=0 = perfect prediction
- Random scatter = good model
- **Our result:** Random scatter, no patterns âœ…

### 3. Residual Distribution
- Histogram of prediction errors
- Should be centered around 0
- Bell-shaped = normal distribution
- **Our result:** Centered at 0, well-distributed âœ…

### 4. Error Distribution by Range
- Box plots showing errors across different wellness ranges
- Helps identify if model performs differently for different ranges
- **Our result:** Consistent performance across ranges âœ…

### 5. Feature Importance
- Bar chart of top 20 most important features
- Shows which features drive predictions
- **Top features:** stress-productivity interaction, health score

---

## ðŸŽ“ For Your Final Year Project Report

### Key Points to Highlight:

**1. Problem Statement:**
"Predicting mental wellness (0-100 index) from screen time and lifestyle data using machine learning."

**2. Methodology:**
- Advanced feature engineering (13 new features)
- Multiple algorithm comparison (10 models)
- Hyperparameter optimization (RandomizedSearchCV)
- Ensemble methods (Voting & Stacking)
- Rigorous evaluation (10-fold cross-validation)

**3. Results:**
- **94.26% RÂ² Score** - Excellent predictive power
- **MAE of 4.02** - Average error of ~4 points on 0-100 scale
- **No overfitting** - Model generalizes well
- **Robust** - Consistent performance across validation folds

**4. Innovation:**
- Comprehensive feature engineering
- Advanced ensemble techniques
- Production-ready implementation
- Thorough documentation

**5. Impact:**
- Can help identify individuals at risk of poor mental wellness
- Provides insights into key factors affecting mental health
- Can guide interventions (e.g., reduce screen time, improve sleep)

---

## ðŸ† Distinction-Level Checklist

âœ… **Advanced Techniques Used:**
- [x] Multiple ML algorithms
- [x] Hyperparameter tuning
- [x] Ensemble learning
- [x] Cross-validation
- [x] Feature engineering
- [x] Outlier handling
- [x] Stratified splitting

âœ… **Professional Implementation:**
- [x] Clean, modular code
- [x] Comprehensive documentation
- [x] Error handling
- [x] Logging & progress tracking
- [x] Saved artifacts for reproducibility

âœ… **Thorough Evaluation:**
- [x] Multiple metrics (RÂ², MAE, RMSE, MAPE)
- [x] Overfitting analysis
- [x] Residual analysis
- [x] Feature importance
- [x] Professional visualizations

âœ… **Academic Rigor:**
- [x] Systematic methodology
- [x] Reproducible results
- [x] Detailed reports
- [x] Clear documentation

---

## ðŸ› Troubleshooting

### Import Error: "No module named 'ai'"
**Solution:**
```bash
$env:PYTHONPATH="$PWD"
```
Run this before executing Python scripts.

### Missing Dependencies
**Solution:**
```bash
pip install pandas numpy scikit-learn matplotlib seaborn joblib
```

### File Not Found Errors
**Solution:** Ensure you're running from the project root directory:
```bash
cd D:\Python Projects\WellSync
```

---

## ðŸ“ž Quick Commands Reference

```bash
# Activate virtual environment
.\.venv\Scripts\Activate.ps1

# Set Python path
$env:PYTHONPATH="$PWD"

# Train model
python ai/src/mental_health/train.py

# Evaluate model
python ai/src/mental_health/evaluate.py

# Check saved models
Get-ChildItem ai/models/mental_health -Recurse

# View visualizations
Start-Process ai/models/mental_health/visualizations/evaluation_predictions.png
```

---

## ðŸŽ¯ Expected Outcomes

After running the pipeline, you should have:

1. âœ… **9 trained models** saved as .pkl files
2. âœ… **5 visualizations** in PNG format
3. âœ… **2 comprehensive reports** in TXT format
4. âœ… **RÂ² Score > 0.94** on test set
5. âœ… **No overfitting** (train-test difference < 0.05)
6. âœ… **Professional-quality outputs** ready for project submission

---

## ðŸ“š References for Report

**Key Techniques Used:**
1. **Feature Engineering** - Creating meaningful features from raw data
2. **Random Forest** - Ensemble of decision trees
3. **Gradient Boosting** - Sequential ensemble learning
4. **Voting Ensemble** - Averaging predictions from multiple models
5. **Stacking** - Meta-learning from base model predictions
6. **Cross-Validation** - K-fold validation for robustness
7. **Hyperparameter Tuning** - RandomizedSearchCV optimization

**Metrics:**
- **RÂ² Score** - Proportion of variance explained (0-1, higher better)
- **MAE** - Mean Absolute Error (lower better)
- **RMSE** - Root Mean Square Error (lower better)

---

**ðŸŽ“ PROJECT STATUS: DISTINCTION-LEVEL READY âœ…**

*All components are implemented with advanced techniques and professional quality suitable for achieving distinction-level grades.*
